This repository contains results, training/testing samples and code used in the computational model sections.

<h2> Results/ </h2>
We used 100 model with randomly initialized weights, 50 of them are directly used to test on the Which-Is-N task without training and 50 of them are trained using numbers used in two kid's behavior studies. The 50 untrained models will be refered to as <b> untrained/ </b> and the 50 trained models will be refered as <b> kid_trained/ </b>.

We used two different measure to compute this model's performance on the Which-Is-N task:
<ol>  Raw Probabilities for all target word generated by the model. These results are in <b> Results/Edit_distance/. </b> </ol>
<ol>  Edit distance to compare between model generated target/foil label and true target label <b> Results/Probabilities. </b> </ol>

We also include probability of which region (left or right) the model's attention tend to fall in when generating each word. These files are in <b> Results/Attention_measure/ </b>


<h2> Training_and_testing_items/ </h2>
Raw images are in <b> images_raw/ </b> for training/validation/testing numbers.
CSV files are in <b> csv/ </b> for training/validation/testing numbers.
A few steps of pre-processing is necessary for the model to be able to intake these numbers, so the processed files for running the models are in <b> images_processed </b>.


<h2> Code/ </h2>
This folder includes all the code we used for training, testing as well as generating result files.
The model and training procedure here are based on code from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning. These scripts are adapted from aforementioned repo:


Python 3.6 and PyTorch 0.4.1 are used for these scripts.
<h4> Training </h4>
python train_bootstrap.py -m [PATH_TO_MODEL_OUTPUT] -d ../Training_and_testing_items/images_processed/ -n [NUN_OF_MODELS]

<h4> Testing Trained Models </h4>
Test newly trained models: 
python caption_bootstrap.py -m [PATH_TO_SAVED_MODELS] -wm ../Training_and_testing_items/images_processed/WORDMAP_coco_1_cap_per_img_1_min_word_freq.json -i ../Training_and_testing_items/images_raw/test_new/ -n 50 \
-oa [PATH_TO_SAVE_RAW_ATTENTION] -g ../Training_and_testing_items/images_raw/test_new/num_labels.json \ 
-t ../Training_and_testing_items/csv/test_trial_new.csv -ot [PATH_TO_SAVE_TRIAL_LEVEL_RESULTS] \ 
 -r [PATH_TO/OVERALL_ACC.csv] -st [new or old] -et [edit or prob] -op [PATH_TO_SAVE_RAW_PROBABILITY]
Test our trained models, please download our models <a href=https://drive.google.com/file/d/1MBrFSgdgnFxzYc12WfcCRN31d-rdrxk4/view?usp=sharing> here </a> and then run the above command.


<h1> Below are the README.md content from https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning. More information can be found there. </h1>
